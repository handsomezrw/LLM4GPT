import os
import math
import json
import torch
import gc
import nltk
import numpy as np
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import jieba
from rouge_score import rouge_scorer
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling
)
# æ–°å¢ï¼šå¼•å…¥PeftModelç”¨äºåŠ è½½LoRAæƒé‡
from peft import PeftModel
from data_preparation import load_custom_dataset, prepare_tokenized_dataset

# ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®ï¼ˆè‹¥æœªä¸‹è½½è¿‡ï¼Œå–æ¶ˆæ³¨é‡Šï¼‰
# nltk.download('punkt_tab')


def clear_gpu_memory():
    """æ¸…ç†GPUæ˜¾å­˜"""
    torch.cuda.empty_cache()
    gc.collect()


def load_lora_model(base_model_path, lora_model_path):
    """
    åŠ è½½â€œåŸå§‹åŸºåº§æ¨¡å‹ + LoRAæƒé‡â€ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
    :param base_model_path: åŸå§‹åŸºåº§æ¨¡å‹è·¯å¾„ï¼ˆå¦‚Qwen-7Bã€Llama-2-7Bç­‰ï¼Œéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    :param lora_model_path: è®­ç»ƒå¥½çš„LoRAæƒé‡è·¯å¾„ï¼ˆå³./lora_modelæ–‡ä»¶å¤¹ï¼‰
    :return: åŠ è½½LoRAåçš„å®Œæ•´æ¨¡å‹ã€tokenizer
    """
    # 1. å…ˆåŠ è½½åŸå§‹åŸºåº§æ¨¡å‹çš„tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        trust_remote_code=True,
        padding_side="right"  # å³paddingï¼Œé¿å…ç”Ÿæˆæ—¶è­¦å‘Š
    )
    # è¡¥å……pad_tokenï¼ˆè‹¥åŸºåº§æ¨¡å‹æ— pad_tokenï¼‰
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    # 2. åŠ è½½åŸå§‹åŸºåº§æ¨¡å‹ï¼ˆä¸è®­ç»ƒæ—¶çš„é‡åŒ–é…ç½®ä¸€è‡´ï¼š8bité‡åŒ–ã€BF16ï¼‰
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡ï¼ˆCPU/GPUï¼‰
        trust_remote_code=True,
        load_in_8bit=True  # ä¿æŒä¸è®­ç»ƒæ—¶ä¸€è‡´çš„8bité‡åŒ–ï¼Œé™ä½æ˜¾å­˜å ç”¨
    )

    # 3. å åŠ LoRAæƒé‡ï¼ˆå…³é”®æ­¥éª¤ï¼šå°†è®­ç»ƒå¥½çš„LoRAé€‚é…å±‚åŠ è½½åˆ°åŸºåº§æ¨¡å‹ï¼‰
    lora_model = PeftModel.from_pretrained(
        base_model,
        model_id=lora_model_path,
        device_map="auto"  # ä¸åŸºåº§æ¨¡å‹è®¾å¤‡ä¸€è‡´
    )

    # 4. åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨Dropoutï¼Œç¡®ä¿ç»“æœç¨³å®šï¼‰
    lora_model.eval()
    return lora_model, tokenizer


def compute_metrics(predictions, labels, tokenizer):
    """è®¡ç®—BLEUã€ROUGE-Lã€æŸå¤±å’Œå›°æƒ‘åº¦æŒ‡æ ‡ï¼ˆé€»è¾‘ä¸å˜ï¼Œä¿ç•™åŸæœ‰ä¼˜åŒ–ï¼‰"""
    rouge_scorer_instance = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    bleu_smoother = SmoothingFunction().method4

    decoded_preds = []
    decoded_labels = []

    # 1. è§£ç æ–‡æœ¬ï¼ˆNumPy argmaxç”¨axisï¼Œä¿®å¤åŸé”™è¯¯ï¼‰
    for pred_logits, label in zip(predictions, labels):
        pred_ids = pred_logits.argmax(axis=-1)  # NumPyæ•°ç»„ç”¨axisï¼Œè€ŒéPyTorchçš„dim
        # è§£ç é¢„æµ‹æ–‡æœ¬
        pred_text = tokenizer.decode(
            pred_ids,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )
        decoded_preds.append(pred_text)
        # è§£ç çœŸå®æ ‡ç­¾ï¼ˆè¿‡æ»¤-100ï¼‰
        label_filtered = [l for l in label if l != -100]
        label_text = tokenizer.decode(
            label_filtered,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )
        decoded_labels.append(label_text)

    # 2. è®¡ç®—BLEUåˆ†æ•°ï¼ˆä¸­æ–‡jiebaåˆ†è¯ï¼‰
    bleu_scores = []
    for pred_text, label_text in zip(decoded_preds, decoded_labels):
        pred_tokens = jieba.lcut(pred_text.strip())
        label_tokens = [jieba.lcut(label_text.strip())]  # BLEUè¦æ±‚å‚è€ƒæ–‡æœ¬ä¸ºåˆ—è¡¨çš„åˆ—è¡¨
        bleu = sentence_bleu(
            label_tokens,
            pred_tokens,
            smoothing_function=bleu_smoother,
            weights=(0.25, 0.25, 0.25, 0.25)  # 4-gramæƒé‡ï¼Œå¹³è¡¡é•¿çŸ­æ–‡æœ¬
        )
        bleu_scores.append(bleu)
    avg_bleu = np.mean(bleu_scores) * 100

    # 3. è®¡ç®—ROUGE-Låˆ†æ•°ï¼ˆå…³æ³¨è¯­ä¹‰ç»“æ„åŒ¹é…ï¼‰
    rouge_scores = []
    for pred_text, label_text in zip(decoded_preds, decoded_labels):
        rouge_result = rouge_scorer_instance.score(label_text, pred_text)
        rouge_scores.append(rouge_result['rougeL'].fmeasure)
    avg_rouge_l = np.mean(rouge_scores) * 100

    # 4. è®¡ç®—æŸå¤±å’Œå›°æƒ‘åº¦ï¼ˆNumPyæ•°ç»„æ“ä½œä¼˜åŒ–ï¼‰
    # æ‹¼æ¥logitsï¼ˆpredictionsæ˜¯listï¼Œæ¯ä¸ªå…ƒç´ ä¸º[seq_len, vocab_size]ï¼‰
    predictions_np = np.concatenate(predictions, axis=0)
    # æ‹¼æ¥æ ‡ç­¾ï¼ˆè¿‡æ»¤-100ï¼‰
    labels_filtered = [label[label != -100] for label in labels]
    labels_np = np.concatenate(labels_filtered, axis=0)
    # ç”Ÿæˆæœ‰æ•ˆæ ‡ç­¾æ©ç ï¼ˆè¿‡æ»¤-100ï¼‰
    all_labels_flat = np.concatenate(labels, axis=0)
    mask = all_labels_flat != -100
    pred_flat_filtered = predictions_np[mask]

    # è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼ˆè½¬ä¸ºPyTorchå¼ é‡ï¼‰
    loss = torch.nn.functional.cross_entropy(
        torch.tensor(pred_flat_filtered, dtype=torch.float32),
        torch.tensor(labels_np, dtype=torch.long)
    ).item()
    # è®¡ç®—å›°æƒ‘åº¦ï¼ˆé¿å…æº¢å‡ºï¼‰
    perplexity = math.exp(loss) if loss < 10 else float('inf')

    # è¿”å›æŒ‡æ ‡å’Œæ ·æœ¬ç»“æœ
    return {
        "metrics": {
            "bleu_score": round(avg_bleu, 2),
            "rouge_l_score": round(avg_rouge_l, 2),
            "test_loss": round(loss, 4),
            "perplexity": round(perplexity, 2)
        },
        "samples": {
            "predictions_text": decoded_preds[:20],
            "labels_text": decoded_labels[:20]
        }
    }


def evaluate_on_test_set(model, tokenizer, test_dataset, output_dir):
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼ˆé€»è¾‘ä¸å˜ï¼Œç¡®ä¿æ‰¹æ¬¡å¤„ç†å’Œå†…å­˜æ¸…ç†ï¼‰"""
    os.makedirs(output_dir, exist_ok=True)
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    print("å¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°...")
    # åˆå§‹åŒ–ç´¯åŠ å˜é‡ï¼ˆé¿å…å­˜å‚¨å…¨éƒ¨logitsï¼Œé™ä½å†…å­˜å ç”¨ï¼‰
    total_bleu = 0.0
    total_rouge_l = 0.0
    total_loss = 0.0
    total_token = 0
    sample_count = 0
    decoded_preds = []
    decoded_labels = []

    # å°æ‰¹æ¬¡å¤„ç†ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼Œ2ä¸ºåŸºç¡€å€¼ï¼Œæ˜¾å­˜ä¸è¶³å¯æ”¹ä¸º1ï¼‰
    batch_size = 2
    for i in range(0, len(test_dataset), batch_size):
        batch = test_dataset[i:i + batch_size]
        # å‡†å¤‡è¾“å…¥ï¼ˆè½¬ä¸ºTensorå¹¶åˆ†é…åˆ°æ¨¡å‹è®¾å¤‡ï¼‰
        inputs = {
            "input_ids": torch.tensor(batch["input_ids"]).to(model.device),
            "attention_mask": torch.tensor(batch["attention_mask"]).to(model.device),
            "labels": torch.tensor(batch["labels"]).to(model.device)
        }

        # æ— æ¢¯åº¦æ¨ç†ï¼ˆè¯„ä¼°é˜¶æ®µç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœæ˜¾å­˜ï¼‰
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits.to(dtype=torch.float32)  # BF16è½¬FP32ï¼Œé¿å…NumPyä¸å…¼å®¹
            batch_loss = outputs.loss.item()  # ç›´æ¥ç”¨æ¨¡å‹è¾“å‡ºçš„æŸå¤±ï¼ˆå·²è‡ªåŠ¨è¿‡æ»¤-100ï¼‰

        # 1. ç´¯åŠ æŸå¤±ï¼ˆæŒ‰æœ‰æ•ˆtokenæ•°åŠ æƒï¼Œç¡®ä¿å¹³å‡æŸå¤±å‡†ç¡®ï¼‰
        batch_token_count = (inputs["labels"] != -100).sum().item()
        total_loss += batch_loss * batch_token_count
        total_token += batch_token_count

        # 2. è§£ç æ–‡æœ¬å¹¶ç´¯åŠ BLEU/ROUGE-L
        pred_ids = logits.argmax(axis=-1).cpu().numpy()  # è½¬NumPyæ•°ç»„ç”¨äºè§£ç 
        label_ids = inputs["labels"].cpu().numpy()

        for pred_id, label_id in zip(pred_ids, label_ids):
            # è§£ç é¢„æµ‹æ–‡æœ¬å’ŒçœŸå®æ ‡ç­¾
            pred_text = tokenizer.decode(pred_id, skip_special_tokens=True)
            label_text = tokenizer.decode([l for l in label_id if l != -100], skip_special_tokens=True)
            decoded_preds.append(pred_text)
            decoded_labels.append(label_text)  # åŸä»£ç ç¬”è¯¯ï¼šæ­¤å¤„åº”å­˜label_textï¼Œépred_textï¼Œå·²ä¿®æ­£
            sample_count += 1

            # è®¡ç®—å•æ ·æœ¬BLEU
            pred_tokens = jieba.lcut(pred_text.strip())
            label_tokens = [jieba.lcut(label_text.strip())]
            total_bleu += sentence_bleu(
                label_tokens,
                pred_tokens,
                smoothing_function=SmoothingFunction().method4
            )

            # è®¡ç®—å•æ ·æœ¬ROUGE-L
            total_rouge_l += rouge_scorer.RougeScorer(
                ['rougeL'], use_stemmer=True
            ).score(label_text, pred_text)['rougeL'].fmeasure

        # æ¸…ç†å½“å‰æ‰¹æ¬¡çš„æ˜¾å­˜
        clear_gpu_memory()
        # æ‰“å°è¿›åº¦
        print(f"å·²å¤„ç† {min(i + batch_size, len(test_dataset))}/{len(test_dataset)} ä¸ªæ ·æœ¬")

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_bleu = (total_bleu / sample_count) * 100 if sample_count > 0 else 0.0
    avg_rouge_l = (total_rouge_l / sample_count) * 100 if sample_count > 0 else 0.0
    avg_loss = total_loss / total_token if total_token > 0 else 0.0
    perplexity = math.exp(avg_loss) if avg_loss < 10 else float('inf')

    # æ•´ç†ç»“æœ
    results = {
        "metrics": {
            "bleu_score": round(avg_bleu, 2),
            "rouge_l_score": round(avg_rouge_l, 2),
            "test_loss": round(avg_loss, 4),
            "perplexity": round(perplexity, 2)
        },
        "samples": {
            "predictions_text": decoded_preds[:20],  # ä»…ä¿ç•™å‰20æ¡æ ·æœ¬ï¼Œé¿å…ç»“æœæ–‡ä»¶è¿‡å¤§
            "labels_text": decoded_labels[:20]
        }
    }

    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    with open(os.path.join(output_dir, "test_metrics.json"), "w", encoding="utf-8") as f:
        json.dump(results["metrics"], f, ensure_ascii=False, indent=2)
    with open(os.path.join(output_dir, "test_samples.json"), "w", encoding="utf-8") as f:
        json.dump(results["samples"], f, ensure_ascii=False, indent=2)

    # æ‰“å°æœ€ç»ˆç»“æœ
    print("\n===== æµ‹è¯•é›†è¯„ä¼°ç»“æœ =====")
    print(f"BLEUåˆ†æ•°: {results['metrics']['bleu_score']}")
    print(f"ROUGE-Låˆ†æ•°: {results['metrics']['rouge_l_score']}")
    print(f"æµ‹è¯•æŸå¤±: {results['metrics']['test_loss']}")
    print(f"å›°æƒ‘åº¦: {results['metrics']['perplexity']}")
    print(f"\nç»“æœå·²ä¿å­˜è‡³ {output_dir}")
    return results
# åŸå§‹æ¨¡å‹
def load_base_model(base_model_path):
    """åªåŠ è½½åŸå§‹åŸºåº§æ¨¡å‹ï¼ˆä¸åŠ è½½LoRAæƒé‡ï¼‰"""
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        trust_remote_code=True,
        padding_side="right"
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        load_in_8bit=True  # å¯é€‰ï¼šæ˜¾å­˜ä¸è¶³æ—¶å¯ç”¨
    )

    base_model.eval()
    return base_model, tokenizer


def main():
    # --------------------------
    # æ ¸å¿ƒé…ç½®ï¼šéœ€æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹ï¼ï¼ï¼
    # --------------------------
    base_model_path = "."  # åŸå§‹åŸºåº§æ¨¡å‹è·¯å¾„ï¼ˆå¦‚Qwen-7Bã€Llama-2-7Bï¼Œéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    lora_model_path = "./data_r8_attention"  # è®­ç»ƒå¥½çš„LoRAæƒé‡è·¯å¾„ï¼ˆå³æœ€ç»ˆä¿å­˜çš„lora_modelæ–‡ä»¶å¤¹ï¼‰
    data_path = "dataset_with_think.jsonl"  # æµ‹è¯•ç”¨æ•°æ®é›†è·¯å¾„ï¼ˆä¸è®­ç»ƒæ—¶çš„æ•°æ®é›†æ ¼å¼ä¸€è‡´ï¼‰
    output_dir = "./test/data_r8_attention"  # æµ‹è¯•ç»“æœä¿å­˜ç›®å½•ï¼ˆåŒºåˆ†åŸoutputï¼Œé¿å…è¦†ç›–ï¼‰
    max_seq_length = 1024  # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼Œå¦åˆ™é¢„å¤„ç†ä¸å…¼å®¹ï¼‰

    # æ¸…ç†åˆå§‹æ˜¾å­˜
    clear_gpu_memory()

    # 1. åŠ è½½æ•°æ®é›†ï¼ˆåŒ…å«train/val/testï¼Œåç»­ä»…ç”¨testé›†ï¼‰
    print("åŠ è½½æ•°æ®é›†...")
    dataset = load_custom_dataset(data_path)

    # ğŸš« ä¸åŠ è½½LoRA
    # print(f"åŠ è½½åŸå§‹æ¨¡å‹: {base_model_path}")
    # model, tokenizer = load_base_model(base_model_path)

    # 2. åŠ è½½â€œåŸºåº§æ¨¡å‹ + LoRAæƒé‡â€ï¼ˆæ ¸å¿ƒä¿®æ”¹æ­¥éª¤ï¼‰
    print(f"åŠ è½½åŸºåº§æ¨¡å‹: {base_model_path}")
    print(f"åŠ è½½LoRAæƒé‡: {lora_model_path}")
    model, tokenizer = load_lora_model(base_model_path, lora_model_path)

    # 3. é¢„å¤„ç†æµ‹è¯•é›†ï¼ˆä¸è®­ç»ƒæ—¶çš„é¢„å¤„ç†é€»è¾‘ä¸€è‡´ï¼‰
    print("é¢„å¤„ç†æµ‹è¯•é›†...")
    tokenized_dataset = prepare_tokenized_dataset(
        dataset,
        tokenizer,
        max_seq_length
    )
    test_dataset = tokenized_dataset["test"]  # æå–æµ‹è¯•é›†

    # 4. åœ¨æµ‹è¯•é›†ä¸Šæ‰§è¡Œè¯„ä¼°
    evaluate_on_test_set(model, tokenizer, test_dataset, output_dir)

    # æ¸…ç†æœ€ç»ˆæ˜¾å­˜
    clear_gpu_memory()
    print("\næµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()